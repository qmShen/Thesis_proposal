\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Title Page}{\nbi i}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Authorization Page}{\nbi ii}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Signature Page}{\nbi iii}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Table of Contents}{\nbi iv}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Figures}{\nbi viii}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{\nbi xi}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Abstract}{\nbi xii}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{{Chapter 1}\hspace  {1em}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {1pt}}
\@writefile{lot}{\addvspace {1pt}}
\newlabel{chap:intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Visualization meets urban informatics}{1}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contribution}{1}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis organization}{1}{section.1.4}\protected@file@percent }
\citation{long_2016_human-scale}
\citation{gehl_1971_life}
\@writefile{toc}{\contentsline {chapter}{{Chapter 2}\hspace  {1em}Visual Exploration of Human-Scale Urban Forms Based on Street Views}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {1pt}}
\@writefile{lot}{\addvspace {1pt}}
\newlabel{chap:c1_intro}{{2}{2}{Visual Exploration of Human-Scale Urban Forms Based on Street Views}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{2}{section.2.1}\protected@file@percent }
\citation{anguelov2010google}
\citation{rundle_2011_using}
\citation{li_2015_accessing}
\citation{Naik_2014_streetscore}
\citation{li_2015_accessing}
\citation{rundle_2011_using}
\citation{Naik_2014_streetscore,li_2015_accessing}
\citation{long_2017_how}
\citation{liu_2015_understanding}
\citation{sun_2013_survey}
\citation{anguelov2010google}
\citation{rundle_2011_using}
\citation{li_2015_accessing}
\citation{Naik_2014_streetscore}
\citation{li_2015_accessing}
\citation{rundle_2011_using,Naik_2014_streetscore,li_2015_accessing}
\citation{doersch2015makes}
\citation{gebru2017using}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Related Work}{4}{section.2.2}\protected@file@percent }
\newlabel{sec:c1_related_work}{{2.2}{4}{Related Work}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Street View Analysis}{4}{subsection.2.2.1}\protected@file@percent }
\citation{Badrinarayanan_2015_segnet}
\citation{ferreira_visual_2013,wang_2013_visual}
\citation{xu_2013_visual,chen_2015_interactive}
\citation{ferreira_2011_birdvis}
\citation{vanegas_2009_visualization}
\citation{zheng_2016_visual}
\citation{qu_2007_visual}
\citation{ortner_2016_vis-a-ware}
\citation{arietta_2014_city}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Urban Data Visualization}{5}{subsection.2.2.2}\protected@file@percent }
\citation{heinrich_2012_state}
\citation{zhou2008visual,holten_2010_evaluation}
\citation{fua1999hierarchical,zhao_2012_structure}
\citation{turkay_2014_attribute}
\citation{goodwin_2016_visualizing}
\citation{turkay_2014_attribute}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Multivariate Geographical Data Visualization}{6}{subsection.2.2.3}\protected@file@percent }
\citation{gleicher_2011_visual}
\citation{yang_2017_blockwise}
\citation{zeng_2017_visualizing}
\citation{long_2016_human-scale}
\citation{jacobs_1961_life,gehl_1971_life}
\citation{gehl_1971_life}
\citation{rundle_2011_using,li_2015_accessing,Naik_2014_streetscore}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Comparative Visualization}{7}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Background and Analytical Tasks}{7}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Background}{7}{subsection.2.3.1}\protected@file@percent }
\newlabel{sec:c1_bg}{{2.3.1}{7}{Background}{subsection.2.3.1}{}}
\citation{jacobs_1961_life,gehl_1971_life}
\citation{gsv_api}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Analytical Tasks}{9}{subsection.2.3.2}\protected@file@percent }
\newlabel{sec:c1_tasks}{{2.3.2}{9}{Analytical Tasks}{subsection.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Overview of StreetVizor workflow. Our system consists of two phases: data modeling and interactive visual exploration.}}{10}{figure.2.1}\protected@file@percent }
\newlabel{fig:c1_sys_overview}{{2.1}{10}{Overview of StreetVizor workflow. Our system consists of two phases: data modeling and interactive visual exploration}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}System Framework}{10}{section.2.4}\protected@file@percent }
\citation{osm_api}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Illustration of data preprocessing: sampling locations in New York City are generated from OpenStreetMap (left), a street view image is collected from Google Street View (center), and the image pixels are classified into six features using SegNet (right).}}{11}{figure.2.2}\protected@file@percent }
\newlabel{fig:c1_data_preprocess}{{2.2}{11}{Illustration of data preprocessing: sampling locations in New York City are generated from OpenStreetMap (left), a street view image is collected from Google Street View (center), and the image pixels are classified into six features using SegNet (right)}{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Data Modeling}{11}{section.2.5}\protected@file@percent }
\citation{Badrinarayanan_2015_segnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Data Collection}{12}{subsection.2.5.1}\protected@file@percent }
\newlabel{ssec:c1_data_collection}{{2.5.1}{12}{Data Collection}{subsection.2.5.1}{}}
\newlabel{c1_eq_sv}{{2.1}{12}{Data Collection}{equation.2.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Feature Extraction}{12}{subsection.2.5.2}\protected@file@percent }
\newlabel{ssec:c1_feature}{{2.5.2}{12}{Feature Extraction}{subsection.2.5.2}{}}
\newlabel{c1_eq_fm}{{2.2}{13}{Feature Extraction}{equation.2.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Data Querying and Filtering}{13}{subsection.2.5.3}\protected@file@percent }
\newlabel{ssec:c1_query}{{2.5.3}{13}{Data Querying and Filtering}{subsection.2.5.3}{}}
\citation{shneiderman1996eyes}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Data model: street views with six-dimensional features of \textit  {greenery, sky, building, road, vehicle} and \textit  {others}, are organized in an octree structure and a street lookup table.}}{14}{figure.2.3}\protected@file@percent }
\newlabel{fig:c1_data_model}{{2.3}{14}{Data model: street views with six-dimensional features of \textit {greenery, sky, building, road, vehicle} and \textit {others}, are organized in an octree structure and a street lookup table}{figure.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Visualization Design}{14}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Design Rationales}{14}{subsection.2.6.1}\protected@file@percent }
\citation{liu_2017_smartAdP}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces StreetVizor system. (a) Control panel enables multi-scale navigation, ranking exploration, and feature filtering. (b) Side-by-side map views compare the spatial distribution of human-scale urban forms in two areas-of-interest (AOIs). (c) AOI statistic view presents the quantitative measurements, including correlation, histogram, and diversity in the AOIs shown in (b). (d) Street map views present detailed street views along two streets. (e) Street statistic view extends parallel coordinates with street layouts.}}{15}{figure.2.4}\protected@file@percent }
\newlabel{fig:c1_teaser}{{2.4}{15}{StreetVizor system. (a) Control panel enables multi-scale navigation, ranking exploration, and feature filtering. (b) Side-by-side map views compare the spatial distribution of human-scale urban forms in two areas-of-interest (AOIs). (c) AOI statistic view presents the quantitative measurements, including correlation, histogram, and diversity in the AOIs shown in (b). (d) Street map views present detailed street views along two streets. (e) Street statistic view extends parallel coordinates with street layouts}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Ranking Explorer}{16}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}AOI Explorer}{16}{subsection.2.6.3}\protected@file@percent }
\newlabel{ssec:c1_aoi_explorer}{{2.6.3}{16}{AOI Explorer}{subsection.2.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{AOI Map View}{16}{section*.4}\protected@file@percent }
\newlabel{sssec:c1_aoi_map_view}{{2.6.3}{16}{AOI Map View}{section*.4}{}}
\citation{scheepens_2011_composite}
\citation{comaniciu_2002_meanshift}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The AOI Statistic View combines (a) a scatterplot matrix to show pair-wise correlations between two features, (b) small multiples of histogram bar charts to overview feature distributions, and (c) small multiples of deviation plots to present feature diversity. }}{17}{figure.2.5}\protected@file@percent }
\newlabel{fig:c1_statistic_view}{{2.5}{17}{The AOI Statistic View combines (a) a scatterplot matrix to show pair-wise correlations between two features, (b) small multiples of histogram bar charts to overview feature distributions, and (c) small multiples of deviation plots to present feature diversity}{figure.2.5}{}}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsubsection}{AOI Statistic View}{18}{section*.5}\protected@file@percent }
\newlabel{sssec:c1_statistic_view}{{2.6.3}{18}{AOI Statistic View}{section*.5}{}}
\citation{chen_2014_visual}
\citation{shneiderman_1992_tree}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Street Map View provides an overview of all street views along a street as colored points, and highlights images on two sides of the street. (a) A tree map showing the feature composition of an image will pop up when the mouse pointer is hovered over the image.}}{20}{figure.2.6}\protected@file@percent }
\newlabel{fig:c1_street_map}{{2.6}{20}{Street Map View provides an overview of all street views along a street as colored points, and highlights images on two sides of the street. (a) A tree map showing the feature composition of an image will pop up when the mouse pointer is hovered over the image}{figure.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Street Explorer}{20}{subsection.2.6.4}\protected@file@percent }
\newlabel{ssec:c1_street_explorer}{{2.6.4}{20}{Street Explorer}{subsection.2.6.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Street Map View}{20}{section*.6}\protected@file@percent }
\newlabel{sssec:c1_street_view}{{2.6.4}{20}{Street Map View}{section*.6}{}}
\citation{qu_2007_visual}
\citation{havre_2002_themeriver}
\@writefile{toc}{\contentsline {subsubsection}{Street Statistic View}{21}{section*.7}\protected@file@percent }
\newlabel{sssec:c1_street_stat_view}{{2.6.4}{21}{Street Statistic View}{section*.7}{}}
\citation{byron_2008_stacked}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Overview of the construction process for Street Statistic View: (a) Construct minimum bounding box of the street network. (b) Rotate the street network such that its bounding box fits in the rendering space. (c) Plot themeriver style visualization within the rendering space. (d) Enhance parallel coordinates with street layouts on both sides.}}{22}{figure.2.7}\protected@file@percent }
\newlabel{fig:c1_street_statistic_view}{{2.7}{22}{Overview of the construction process for Street Statistic View: (a) Construct minimum bounding box of the street network. (b) Rotate the street network such that its bounding box fits in the rendering space. (c) Plot themeriver style visualization within the rendering space. (d) Enhance parallel coordinates with street layouts on both sides}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces  AOI Map View compares spatial distributions of human-scale urban forms in Singapore (left) and Greater London (right). Orange points (buildings) are concentrated around the highlighted center area of Greater London, i.e., City of London.}}{23}{figure.2.8}\protected@file@percent }
\newlabel{fig:c1_study_1_spatial}{{2.8}{23}{AOI Map View compares spatial distributions of human-scale urban forms in Singapore (left) and Greater London (right). Orange points (buildings) are concentrated around the highlighted center area of Greater London, i.e., City of London}{figure.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}User Interactions}{23}{subsection.2.6.5}\protected@file@percent }
\newlabel{ssec:c1_user_interaction}{{2.6.5}{23}{User Interactions}{subsection.2.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces AOI Statistic View in coordination with Fig.\nobreakspace  {}\ref  {fig:c1_study_1_spatial} presents quantitative measurements differences of human-scale urban forms in Singapore (red) and Greater London (blue).}}{24}{figure.2.9}\protected@file@percent }
\newlabel{fig:c1_study_1_statistic}{{2.9}{24}{AOI Statistic View in coordination with Fig.~\ref {fig:c1_study_1_spatial} presents quantitative measurements differences of human-scale urban forms in Singapore (red) and Greater London (blue)}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Top three districts with highest $building$ ratios, while bottom three with highest $greenery$ ratios in Hong Kong.}}{25}{figure.2.10}\protected@file@percent }
\newlabel{fig:s1_region-funcitonality}{{2.10}{25}{Top three districts with highest $building$ ratios, while bottom three with highest $greenery$ ratios in Hong Kong}{figure.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Case Studies}{25}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}City-scale Comparison}{25}{subsection.2.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Region-scale Exploration}{26}{subsection.2.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Region-scale comparison of Tanglin in Singapore with Central Park in New York City.}}{27}{figure.2.11}\protected@file@percent }
\newlabel{fig:c1_center-park-tanglin}{{2.11}{27}{Region-scale comparison of Tanglin in Singapore with Central Park in New York City}{figure.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Street-scale Comparison}{28}{subsection.2.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Street Explorer compares the differences in human-scale urban forms of two streets in Brooklyn, New York City (left) and Kowloon, Hong Kong (right). The left street views contain more balanced features, whereas the right street views are dominated by $building$ and $road$.}}{29}{figure.2.12}\protected@file@percent }
\newlabel{fig:c1_study_3}{{2.12}{29}{Street Explorer compares the differences in human-scale urban forms of two streets in Brooklyn, New York City (left) and Kowloon, Hong Kong (right). The left street views contain more balanced features, whereas the right street views are dominated by $building$ and $road$}{figure.2.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Expert Review}{30}{section.2.8}\protected@file@percent }
\citation{jiang_2014_dose}
\citation{liu_2015_understanding,long_2017_how}
\citation{lins_2013_nanocubes}
\citation{wang_2017_gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Discussion}{32}{section.2.9}\protected@file@percent }
\newlabel{sec:c1_discussion}{{2.9}{32}{Discussion}{section.2.9}{}}
\citation{lins_2013_nanocubes,wang_2017_gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Conclusion and Future Work}{33}{section.2.10}\protected@file@percent }
\citation{dodge_2008_towards}
\citation{brock_2006_scaling}
\citation{wang2012understanding}
\citation{tobler_1987_experiments}
\citation{phan2005flow,verbeek_2011_flow}
\citation{andrienko_spatial_2011-1,guo2014origin}
\@writefile{toc}{\contentsline {chapter}{{Chapter 3}\hspace  {1em}Route-Aware Edge Bundling for Visualizing Origin-Destination Trails in Urban Traffic}{35}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {1pt}}
\@writefile{lot}{\addvspace {1pt}}
\newlabel{chap:c2_intro}{{3}{35}{Route-Aware Edge Bundling for Visualizing Origin-Destination Trails in Urban Traffic}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{35}{section.3.1}\protected@file@percent }
\newlabel{sec:intro}{{3.1}{35}{Introduction}{section.3.1}{}}
\citation{holten2009force,selassie2011divided}
\citation{hurter2012graph,lhuillier2017ffteb}
\citation{holten2006hierarchical,cui2008geometry}
\citation{hurter2012graph}
\citation{chen_survey_2015,andrienko2017visual}
\citation{ferreira2013visual,zeng_2015_visualizing}
\citation{kruger_trajectorylenses_2013,zeng_2015_visualizing}
\citation{wilkinson2009history}
\citation{wood2010visualisation}
\citation{guo2009flow}
\citation{von2016mobilitygraphs}
\citation{phan2005flow}
\citation{cornel_2016_composite}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related Work}{37}{section.3.2}\protected@file@percent }
\newlabel{sec:related}{{3.2}{37}{Related Work}{section.3.2}{}}
\citation{zhou2013edge,lhuillier2017state}
\citation{zhou2013edge}
\citation{holten2009force}
\citation{holten2006hierarchical}
\citation{zhou2008energy,lambert2010winding}
\citation{cui2008geometry}
\citation{hurter_2018_functional}
\citation{ersoy2011skeleton}
\citation{hurter2012graph}
\citation{van2016cubu,lhuillier2017ffteb}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Methods for visualizing OD trails in urban traffic: (a) map matching, (b) vector map, (c) \& (d) KDEEB bundles rendered in density and direction, respectively.}}{38}{figure.3.1}\protected@file@percent }
\newlabel{fig:alternatives}{{3.1}{38}{Methods for visualizing OD trails in urban traffic: (a) map matching, (b) vector map, (c) \& (d) KDEEB bundles rendered in density and direction, respectively}{figure.3.1}{}}
\citation{luo2012ambiguity,bach_2017_towards}
\citation{selassie_2011_divided}
\citation{lambert20103d,thony2015vector}
\citation{2014_bottger_three-d,yang_2017_blockwise}
\citation{thony2015vector}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Problem Statement and System Overview}{39}{section.3.3}\protected@file@percent }
\newlabel{sec:overview}{{3.3}{39}{Problem Statement and System Overview}{section.3.3}{}}
\citation{van2016cubu}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces KDEEB applied to taxi trips in Manhattan with different kernel sizes $p_r$: (a) 120, (b) 80, (c) 40, and (d) 20. More detailed bundles are generated with smaller $p_r$.}}{40}{figure.3.2}\protected@file@percent }
\newlabel{fig:kernel_size}{{3.2}{40}{KDEEB applied to taxi trips in Manhattan with different kernel sizes $p_r$: (a) 120, (b) 80, (c) 40, and (d) 20. More detailed bundles are generated with smaller $p_r$}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}KDEEB Algorithm}{40}{subsection.3.3.1}\protected@file@percent }
\newlabel{ssec:kdeeb}{{3.3.1}{40}{KDEEB Algorithm}{subsection.3.3.1}{}}
\newlabel{eq:kernel_density_estimation}{{3.1}{40}{KDEEB Algorithm}{equation.3.3.1}{}}
\newlabel{eq:advecting_points}{{3.2}{40}{KDEEB Algorithm}{equation.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Overview of RAEB pipeline. Our method mainly consists of three phases: \textit  {Preprocessing} for creating a proper initial layout, \textit  {Bundling} for bundling input OD trails, and \textit  {Evaluation} for generating a stable bundle structure.}}{41}{figure.3.3}\protected@file@percent }
\newlabel{fig:framework}{{3.3}{41}{Overview of RAEB pipeline. Our method mainly consists of three phases: \textit {Preprocessing} for creating a proper initial layout, \textit {Bundling} for bundling input OD trails, and \textit {Evaluation} for generating a stable bundle structure}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Problem Identification}{41}{subsection.3.3.2}\protected@file@percent }
\citation{hurter2012graph}
\citation{van2016cubu}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}RAEB Overview}{42}{subsection.3.3.3}\protected@file@percent }
\citation{ferreira2013visual}
\citation{zeng_2015_visualizing}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Preprocessing}{43}{section.3.4}\protected@file@percent }
\newlabel{sec:preprocess}{{3.4}{43}{Preprocessing}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Basic Traffic Concepts}{43}{subsection.3.4.1}\protected@file@percent }
\citation{kruger_trajectorylenses_2013}
\citation{wang_2014_visual-reasoning}
\citation{lou_2009_map}
\citation{kruger_2018_visual}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Map Matching}{44}{subsection.3.4.2}\protected@file@percent }
\newlabel{section:edge_matching}{{3.4.2}{44}{Map Matching}{subsection.3.4.2}{}}
\citation{wang_2014_visual-reasoning}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Route hierarchy, OSM indicator and hierarchy score}}{45}{table.3.1}\protected@file@percent }
\newlabel{tab:sometab}{{3.1}{45}{Route hierarchy, OSM indicator and hierarchy score}{table.3.1}{}}
\newlabel{table: road_score}{{3.1}{45}{Route hierarchy, OSM indicator and hierarchy score}{table.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Hierarchical Route Structure Construction}{45}{subsection.3.4.3}\protected@file@percent }
\newlabel{ssec:hiera_road}{{3.4.3}{45}{Hierarchical Route Structure Construction}{subsection.3.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Illustration of hierarchical route structure and OD trail abstraction: (a) Three route levels are constructed colored in blue, purple and green, repsectively; a raw OD trail is abstracted in accordance to route (b) level-3, (c) level-2, and (d) level-1.}}{46}{figure.3.4}\protected@file@percent }
\newlabel{fig:road_hierarchy}{{3.4}{46}{Illustration of hierarchical route structure and OD trail abstraction: (a) Three route levels are constructed colored in blue, purple and green, repsectively; a raw OD trail is abstracted in accordance to route (b) level-3, (c) level-2, and (d) level-1}{figure.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Trail Abstraction}{46}{subsection.3.4.4}\protected@file@percent }
\newlabel{section: trail_abstraction}{{3.4.4}{46}{Trail Abstraction}{subsection.3.4.4}{}}
\citation{hurter2012graph}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Bundling Method}{47}{section.3.5}\protected@file@percent }
\newlabel{sec:bundling}{{3.5}{47}{Bundling Method}{section.3.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces KernelSizeMeasurement}}{47}{algorithm.1}\protected@file@percent }
\newlabel{al:kernel_size_measurement}{{1}{47}{Bundling Method}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Optimal Kernel Size}{47}{subsection.3.5.1}\protected@file@percent }
\newlabel{ssec:kernel_size}{{3.5.1}{47}{Optimal Kernel Size}{subsection.3.5.1}{}}
\citation{gudmundsson2011computational}
\citation{eiter_1994_computing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Density Map Generation}{48}{subsection.3.5.2}\protected@file@percent }
\citation{maes1997multimodality}
\newlabel{eq:new_density}{{3.5}{49}{Density Map Generation}{equation.3.5.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Evaluation}{49}{section.3.6}\protected@file@percent }
\newlabel{sec:eva}{{3.6}{49}{Evaluation}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Normalized Mutual Information}{49}{subsection.3.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces (left) Bundling stability $p_{bs}$ measured at each bundling iteration for different decay ratios $\lambda $, (right) visually indistinguishable images are generated at iteration 10 and 11 for $\lambda =0.9$.}}{50}{figure.3.5}\protected@file@percent }
\newlabel{fig:nmi}{{3.5}{50}{(left) Bundling stability $p_{bs}$ measured at each bundling iteration for different decay ratios $\lambda $, (right) visually indistinguishable images are generated at iteration 10 and 11 for $\lambda =0.9$}{figure.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces  Leftmost: raw 100K artificial OD trails on a grid road network in there hierarchies. (a) - (d): RAEB bundling results with route awareness parameter $p_{ra}$ set to 0 - 3, respectively. }}{50}{figure.3.6}\protected@file@percent }
\newlabel{fig:grid}{{3.6}{50}{Leftmost: raw 100K artificial OD trails on a grid road network in there hierarchies. (a) - (d): RAEB bundling results with route awareness parameter $p_{ra}$ set to 0 - 3, respectively}{figure.3.6}{}}
\newlabel{eq:mi}{{3.6}{50}{Normalized Mutual Information}{equation.3.6.6}{}}
\newlabel{eq:nmi}{{3.7}{50}{Normalized Mutual Information}{equation.3.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces  Left: raw 100K artificial OD trails on a hierarchical road network. Right: RAEB bundling result with $p_{ra}$ set to 2. }}{51}{figure.3.7}\protected@file@percent }
\newlabel{fig:hier}{{3.7}{51}{Left: raw 100K artificial OD trails on a hierarchical road network. Right: RAEB bundling result with $p_{ra}$ set to 2}{figure.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Bundle Deviation}{51}{subsection.3.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Density maps of NYC taxi trips: (a) shortest paths mapped onto the road network, (b) KDEEB bundles with kernel size $p_r$ = 60, (c) KDEEB bundles with $p_r$ = 21, and (d) our RAEB bundles with $p_r$ = 21.}}{52}{figure.3.8}\protected@file@percent }
\newlabel{fig:nyc_visual}{{3.8}{52}{Density maps of NYC taxi trips: (a) shortest paths mapped onto the road network, (b) KDEEB bundles with kernel size $p_r$ = 60, (c) KDEEB bundles with $p_r$ = 21, and (d) our RAEB bundles with $p_r$ = 21}{figure.3.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Fine scale density maps of NYC taxi trips in Queens zone generated by (a) KDEEB, and (b) RAEB.}}{52}{figure.3.9}\protected@file@percent }
\newlabel{fig:nyc-zoom}{{3.9}{52}{Fine scale density maps of NYC taxi trips in Queens zone generated by (a) KDEEB, and (b) RAEB}{figure.3.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Applications}{52}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Artificial OD Trails}{52}{subsection.3.7.1}\protected@file@percent }
\newlabel{ssec:study1}{{3.7.1}{52}{Artificial OD Trails}{subsection.3.7.1}{}}
\citation{hurter2012graph}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces  Density maps of Shenzhen taxi trips: (a) raw GPS records are mapped onto road network, (b) KDEEB bundles trips on close aterial roads together, while (c) RAEB preserves these roads. All lines are colored according to the OD directions. }}{53}{figure.3.10}\protected@file@percent }
\newlabel{fig:shenzhen}{{3.10}{53}{Density maps of Shenzhen taxi trips: (a) raw GPS records are mapped onto road network, (b) KDEEB bundles trips on close aterial roads together, while (c) RAEB preserves these roads. All lines are colored according to the OD directions}{figure.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}New York Taxi Trips}{54}{subsection.3.7.2}\protected@file@percent }
\newlabel{ssec:study2}{{3.7.2}{54}{New York Taxi Trips}{subsection.3.7.2}{}}
\citation{lou_2009_map}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Shenzhen Taxi Trips}{55}{subsection.3.7.3}\protected@file@percent }
\newlabel{ssec:study3}{{3.7.3}{55}{Shenzhen Taxi Trips}{subsection.3.7.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Main parameter adaptions in RAEB, in comparison with these in KDEEB.}}{56}{table.3.2}\protected@file@percent }
\newlabel{tab:parameters}{{3.2}{56}{Main parameter adaptions in RAEB, in comparison with these in KDEEB}{table.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Discussion}{56}{section.3.8}\protected@file@percent }
\newlabel{sec:discussion}{{3.8}{56}{Discussion}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Parameters}{56}{subsection.3.8.1}\protected@file@percent }
\newlabel{ssec:parameters}{{3.8.1}{56}{Parameters}{subsection.3.8.1}{}}
\citation{hurter2012graph}
\citation{van2016cubu}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Statistics comparison of KDEEB and RAEB for datasets used in the experiments. }}{58}{table.3.3}\protected@file@percent }
\newlabel{tab:statistics}{{3.3}{58}{Statistics comparison of KDEEB and RAEB for datasets used in the experiments}{table.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Performance}{58}{subsection.3.8.2}\protected@file@percent }
\citation{kruger_trajectorylenses_2013}
\citation{wang_2014_visual-reasoning}
\citation{zeng_2015_visualizing}
\citation{van2016cubu}
\citation{lambert20103d,thony2015vector}
\citation{hurter_2018_functional}
\citation{kwon_2016_study}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.3}Applicability and Limitations}{59}{subsection.3.8.3}\protected@file@percent }
\newlabel{ssec:app}{{3.8.3}{59}{Applicability and Limitations}{subsection.3.8.3}{}}
\citation{andrienko_visual_2012}
\citation{ersoy2011skeleton}
\citation{holten2009force}
\citation{van2016cubu}
\citation{lhuillier2017state}
\citation{2014_bottger_three-d,yang_2017_blockwise}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Conclusion and future work}{60}{section.3.9}\protected@file@percent }
\newlabel{sec:con}{{3.9}{60}{Conclusion and future work}{section.3.9}{}}
\citation{xingjian2015convolutional,cao2012forecasting,shi2017deep}
\citation{oprea2016neural,zhou2017prediction,li2017long}
\@writefile{toc}{\contentsline {chapter}{{Chapter 4}\hspace  {1em}Visual Interpretation of Recurrent Neural Network on Multi-dimensional Time-series Forecast}{62}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {1pt}}
\@writefile{lot}{\addvspace {1pt}}
\newlabel{chap:c3_intro}{{4}{62}{Visual Interpretation of Recurrent Neural Network on Multi-dimensional Time-series Forecast}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{62}{section.4.1}\protected@file@percent }
\newlabel{section:introduction}{{4.1}{62}{Introduction}{section.4.1}{}}
\citation{strobelt2018lstmvis}
\citation{ming2017understanding}
\citation{hochreiter1991untersuchungen}
\citation{hochreiter1997long}
\citation{cho2014learning}
\citation{schuster1997bidirectional}
\citation{greff2017lstm}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Related Work}{64}{section.4.2}\protected@file@percent }
\newlabel{sec:rel}{{4.2}{64}{Related Work}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Recurrent Neural Networks}{64}{subsection.4.2.1}\protected@file@percent }
\newlabel{sec:rnn}{{4.2.1}{64}{Recurrent Neural Networks}{subsection.4.2.1}{}}
\citation{sutskever2014sequence}
\citation{hermann2015teaching}
\citation{xingjian2015convolutional}
\citation{chen2018applications}
\citation{oprea2016neural}
\citation{lipton2017doctor}
\citation{ribeiro2016should}
\citation{craven1996extracting}
\citation{andrews1995survey}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Machine Learning Interpretation}{65}{subsection.4.2.2}\protected@file@percent }
\newlabel{sec:mlinter}{{4.2.2}{65}{Machine Learning Interpretation}{subsection.4.2.2}{}}
\citation{friedman2001greedy}
\citation{lundberg2017unified}
\citation{karpathy2015visualizing}
\citation{strobelt2018lstmvis}
\citation{ming2017understanding}
\citation{strobelt2019s}
\citation{kwon2019retainvis}
\citation{zheng2015forecasting}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Application and Models}{67}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Application}{67}{subsection.4.3.1}\protected@file@percent }
\newlabel{section:application}{{4.3.1}{67}{Application}{subsection.4.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Data Description}{67}{subsection.4.3.2}\protected@file@percent }
\newlabel{section:datadescription}{{4.3.2}{67}{Data Description}{subsection.4.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces There are two types of features taken as input: air pollution and meteorology. }}{67}{table.4.1}\protected@file@percent }
\newlabel{table:feature_list}{{4.1}{67}{There are two types of features taken as input: air pollution and meteorology}{table.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Models Description}{68}{subsection.4.3.3}\protected@file@percent }
\newlabel{section:model_description}{{4.3.3}{68}{Models Description}{subsection.4.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces {\color  {black}{RNN Architectures considered in our experiments: A) RNN: the RNN layer is directly connected to the output layer; B) RNN-Dense: adding dense layers between the RNN layer and the output layer.}} }}{69}{figure.4.1}\protected@file@percent }
\newlabel{fig:model_type}{{4.1}{69}{\UC {RNN Architectures considered in our experiments: A) RNN: the RNN layer is directly connected to the output layer; B) RNN-Dense: adding dense layers between the RNN layer and the output layer.}}{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}System design}{69}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Task analysis}{69}{subsection.4.4.1}\protected@file@percent }
\newlabel{section:design_tasks}{{4.4.1}{69}{Task analysis}{subsection.4.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The system overview. There are three major modules in our system: Preprocessing module, Analysis module and Visualization module. }}{70}{figure.4.2}\protected@file@percent }
\newlabel{fig:system_framework}{{4.2}{70}{The system overview. There are three major modules in our system: Preprocessing module, Analysis module and Visualization module}{figure.4.2}{}}
\citation{sun2015deeply}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}System Overview}{71}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Model Interpretation}{71}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Relationships between Hidden States and Features}{71}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Hidden State Distribution}{72}{section*.8}\protected@file@percent }
\newlabel{section:response_and_activation}{{4.5.1}{72}{Hidden State Distribution}{section*.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Compare the response of hidden units(92 and 93) to features ($PM_{2.5}$ and $SO_2$). }}{73}{figure.4.3}\protected@file@percent }
\newlabel{fig:unit_distribution_subgroup}{{4.3}{73}{Compare the response of hidden units(92 and 93) to features ($PM_{2.5}$ and $SO_2$)}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Relationship Strength Estimation}{73}{section*.9}\protected@file@percent }
\newlabel{section:qualify_response}{{4.5.1}{73}{Relationship Strength Estimation}{section*.9}{}}
\citation{ming2017understanding,liu2017towards}
\citation{pezzotti2018deepeyes}
\citation{ming2017understanding}
\citation{dhillon2001co}
\citation{rousseeuw1987silhouettes}
\newlabel{equation:qualify_response}{{4.5}{74}{Relationship Strength Estimation}{equation.4.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Hidden Unit and Feature Clustering}{74}{subsection.4.5.2}\protected@file@percent }
\newlabel{section:clustering}{{4.5.2}{74}{Hidden Unit and Feature Clustering}{subsection.4.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Cluster score with different cluster number. Left: feature cluster. Right: hidden unit cluster. The horizontal axis represents the cluster number, the vertical axis represents the cluster score. }}{75}{figure.4.4}\protected@file@percent }
\newlabel{fig:cluster_parameters}{{4.4}{75}{Cluster score with different cluster number. Left: feature cluster. Right: hidden unit cluster. The horizontal axis represents the cluster number, the vertical axis represents the cluster score}{figure.4.4}{}}
\citation{li2015visualizing}
\citation{li2015visualizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Local Feature Importance}{76}{subsection.4.5.3}\protected@file@percent }
\newlabel{section:feature_importance}{{4.5.3}{76}{Local Feature Importance}{subsection.4.5.3}{}}
\newlabel{equation:feature_gradient}{{4.6}{76}{Local Feature Importance}{equation.4.5.6}{}}
\newlabel{equation:cluster_gradient}{{4.7}{76}{Local Feature Importance}{equation.4.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Visualization Design}{76}{section.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Design of Hidden unit distribution and feature glyph. A) Hidden unit cluster; B) Hidden unit distribution; C) Feature cluster; D) Feature distribution for selected features; E) Feature glyph design; G) Response link MultiRNNExplorer contains multiple coordinated views to support exploring and understanding RNNs' behaviors on multi-dimensional time-series data, especially on hidden unit response and feature importance. The Configuration Panel (B) allows users to select a RNN models and configure parameters. To reveal model mechanism, the Cluster View (A) summarizes the hidden unit clusters' response to feature clusters, and the Feature Importance View (C) summarizes the temporal importance of input features. The Projection View (E) displays a data overview, allowing users to identify and select sequence instances of interest for further analysis. The selected instances will be shown by the Individual View (D). }}{77}{figure.4.5}\protected@file@percent }
\newlabel{fig:teaser}{{4.5}{77}{Design of Hidden unit distribution and feature glyph. A) Hidden unit cluster; B) Hidden unit distribution; C) Feature cluster; D) Feature distribution for selected features; E) Feature glyph design; G) Response link MultiRNNExplorer contains multiple coordinated views to support exploring and understanding RNNs' behaviors on multi-dimensional time-series data, especially on hidden unit response and feature importance. The Configuration Panel (B) allows users to select a RNN models and configure parameters. To reveal model mechanism, the Cluster View (A) summarizes the hidden unit clusters' response to feature clusters, and the Feature Importance View (C) summarizes the temporal importance of input features. The Projection View (E) displays a data overview, allowing users to identify and select sequence instances of interest for further analysis. The selected instances will be shown by the Individual View (D)}{figure.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Cluster View}{77}{subsection.4.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Design of Hidden unit distribution and feature glyph. A) Hidden unit cluster; B) Hidden unit distribution; C) Feature cluster; D) Feature distribution for selected features; E) Feature glyph design; G) Response link}}{78}{figure.4.6}\protected@file@percent }
\newlabel{fig:cluster_design}{{4.6}{78}{Design of Hidden unit distribution and feature glyph. A) Hidden unit cluster; B) Hidden unit distribution; C) Feature cluster; D) Feature distribution for selected features; E) Feature glyph design; G) Response link}{figure.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Feature Importance View}{79}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Projection View}{80}{subsection.4.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Individual View}{80}{subsection.4.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Individual design and the alternative designs. A) Individual View. A1) Feature Trend Chart; A2) Cluster Importance Chart; A3) Top Features Chart. B) themeriver as the alternative design of the Cluster Importance Chart; C) and D) node-like sequence and node sequence as the alternative design of Top Features List.}}{81}{figure.4.7}\protected@file@percent }
\newlabel{fig:individual_view}{{4.7}{81}{Individual design and the alternative designs. A) Individual View. A1) Feature Trend Chart; A2) Cluster Importance Chart; A3) Top Features Chart. B) themeriver as the alternative design of the Cluster Importance Chart; C) and D) node-like sequence and node sequence as the alternative design of Top Features List}{figure.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.5}Interactions and Linkage}{82}{subsection.4.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Case study}{83}{section.4.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Configuration and performance of RNNs, including vanilla RNN, GRU, LSTM, and the RNNs with dense layer (e.g., RNN-Dense). The performance is evaluated by the mean square error (MSE) of $PM_{2.5}$; low MSE represents better performance.}}{83}{table.4.2}\protected@file@percent }
\newlabel{table:model_configuration}{{4.2}{83}{Configuration and performance of RNNs, including vanilla RNN, GRU, LSTM, and the RNNs with dense layer (e.g., RNN-Dense). The performance is evaluated by the mean square error (MSE) of $PM_{2.5}$; low MSE represents better performance}{table.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Discussion and Conclusion}{83}{section.4.8}\protected@file@percent }
\citation{van2017visual,pezzotti2016hierarchical}
\citation{jurgovsky2018sequence}
\bibstyle{plain}
\bibdata{ref}
\bibcite{andrews1995survey}{1}
\bibcite{andrienko2017visual}{2}
\bibcite{andrienko_spatial_2011-1}{3}
\bibcite{andrienko_visual_2012}{4}
\bibcite{anguelov2010google}{5}
\bibcite{arietta_2014_city}{6}
\bibcite{bach_2017_towards}{7}
\bibcite{Badrinarayanan_2015_segnet}{8}
\bibcite{brock_2006_scaling}{9}
\@writefile{toc}{\contentsline {chapter}{References}{85}{chapter*.10}\protected@file@percent }
\bibcite{2014_bottger_three-d}{10}
\bibcite{byron_2008_stacked}{11}
\bibcite{cao2012forecasting}{12}
\bibcite{chen_2014_visual}{13}
\bibcite{chen_2015_interactive}{14}
\bibcite{chen_survey_2015}{15}
\bibcite{chen2018applications}{16}
\bibcite{cho2014learning}{17}
\bibcite{comaniciu_2002_meanshift}{18}
\bibcite{cornel_2016_composite}{19}
\bibcite{craven1996extracting}{20}
\bibcite{cui2008geometry}{21}
\bibcite{dhillon2001co}{22}
\bibcite{dodge_2008_towards}{23}
\bibcite{doersch2015makes}{24}
\bibcite{eiter_1994_computing}{25}
\bibcite{ersoy2011skeleton}{26}
\bibcite{ferreira_2011_birdvis}{27}
\bibcite{ferreira2013visual}{28}
\bibcite{ferreira_visual_2013}{29}
\bibcite{friedman2001greedy}{30}
\bibcite{fua1999hierarchical}{31}
\bibcite{gebru2017using}{32}
\bibcite{gehl_1971_life}{33}
\bibcite{gleicher_2011_visual}{34}
\bibcite{goodwin_2016_visualizing}{35}
\bibcite{gsv_api}{36}
\bibcite{greff2017lstm}{37}
\bibcite{gudmundsson2011computational}{38}
\bibcite{guo2009flow}{39}
\bibcite{guo2014origin}{40}
\bibcite{havre_2002_themeriver}{41}
\bibcite{heinrich_2012_state}{42}
\bibcite{hermann2015teaching}{43}
\bibcite{hochreiter1991untersuchungen}{44}
\bibcite{hochreiter1997long}{45}
\bibcite{holten2006hierarchical}{46}
\bibcite{holten2009force}{47}
\bibcite{holten_2010_evaluation}{48}
\bibcite{hurter_2018_functional}{49}
\bibcite{hurter2012graph}{50}
\bibcite{jacobs_1961_life}{51}
\bibcite{jiang_2014_dose}{52}
\bibcite{jurgovsky2018sequence}{53}
\bibcite{karpathy2015visualizing}{54}
\bibcite{kruger_2018_visual}{55}
\bibcite{kruger_trajectorylenses_2013}{56}
\bibcite{kwon2019retainvis}{57}
\bibcite{kwon_2016_study}{58}
\bibcite{lambert20103d}{59}
\bibcite{lambert2010winding}{60}
\bibcite{lhuillier2017state}{61}
\bibcite{lhuillier2017ffteb}{62}
\bibcite{li2015visualizing}{63}
\bibcite{li2017long}{64}
\bibcite{li_2015_accessing}{65}
\bibcite{lins_2013_nanocubes}{66}
\bibcite{lipton2017doctor}{67}
\bibcite{liu_2017_smartAdP}{68}
\bibcite{liu2017towards}{69}
\bibcite{liu_2015_understanding}{70}
\bibcite{long_2017_how}{71}
\bibcite{long_2016_human-scale}{72}
\bibcite{lou_2009_map}{73}
\bibcite{lundberg2017unified}{74}
\bibcite{luo2012ambiguity}{75}
\bibcite{maes1997multimodality}{76}
\bibcite{ming2017understanding}{77}
\bibcite{Naik_2014_streetscore}{78}
\bibcite{osm_api}{79}
\bibcite{oprea2016neural}{80}
\bibcite{ortner_2016_vis-a-ware}{81}
\bibcite{scikit-learn}{82}
\bibcite{pezzotti2016hierarchical}{83}
\bibcite{pezzotti2018deepeyes}{84}
\bibcite{phan2005flow}{85}
\bibcite{qu_2007_visual}{86}
\bibcite{ribeiro2016should}{87}
\bibcite{rousseeuw1987silhouettes}{88}
\bibcite{rundle_2011_using}{89}
\bibcite{scheepens_2011_composite}{90}
\bibcite{schuster1997bidirectional}{91}
\bibcite{selassie2011divided}{92}
\bibcite{selassie_2011_divided}{93}
\bibcite{shi2017deep}{94}
\bibcite{shneiderman_1992_tree}{95}
\bibcite{shneiderman1996eyes}{96}
\bibcite{strobelt2019s}{97}
\bibcite{strobelt2018lstmvis}{98}
\bibcite{sun_2013_survey}{99}
\bibcite{sun2015deeply}{100}
\bibcite{sutskever2014sequence}{101}
\bibcite{thony2015vector}{102}
\bibcite{tobler_1987_experiments}{103}
\bibcite{turkay_2014_attribute}{104}
\bibcite{van2016cubu}{105}
\bibcite{van2017visual}{106}
\bibcite{vanegas_2009_visualization}{107}
\bibcite{verbeek_2011_flow}{108}
\bibcite{von2016mobilitygraphs}{109}
\bibcite{wang_2014_visual-reasoning}{110}
\bibcite{wang2012understanding}{111}
\bibcite{wang_2017_gaussian}{112}
\bibcite{wang_2013_visual}{113}
\bibcite{wilkinson2009history}{114}
\bibcite{wood2010visualisation}{115}
\bibcite{xingjian2015convolutional}{116}
\bibcite{xu_2013_visual}{117}
\bibcite{yang_2017_blockwise}{118}
\bibcite{zeng_2015_visualizing}{119}
\bibcite{zeng_2017_visualizing}{120}
\bibcite{zhao_2012_structure}{121}
\bibcite{zheng_2016_visual}{122}
\bibcite{zheng2015forecasting}{123}
\bibcite{zhou2013edge}{124}
\bibcite{zhou2008energy}{125}
\bibcite{zhou2008visual}{126}
\bibcite{zhou2017prediction}{127}
